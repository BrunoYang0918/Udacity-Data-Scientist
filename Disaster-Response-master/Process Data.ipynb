{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        1. messages_filepath: path of messages datasets\n",
    "        2. categories_filepath: path of categories datasets\n",
    "    \n",
    "    Output:\n",
    "        1. df: merged dataframe, which contains data from messages, categories files\n",
    "          \n",
    "    Process: \n",
    "        1. Load the required datasets, messages, categories\n",
    "        2. Merge the two datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load messages dataset\n",
    "    messages = pd.read_csv('messages.csv')\n",
    "    \n",
    "    # Load categories dataset\n",
    "    categories = pd.read_csv('categories.csv')\n",
    "    \n",
    "    # Merge datasets\n",
    "    df = pd.merge(messages, categories, on='id', how='inner')\n",
    "    \n",
    "    # Return dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        1. df: the merged dataframed created by the load_data function\n",
    "    \n",
    "    Output:\n",
    "        1. df: cleaned dataframed after processed through the below process\n",
    "    \n",
    "    Process:\n",
    "        1. Create Y columns\n",
    "        2. Rename Y columns\n",
    "        3. Clean Y columns values\n",
    "        4. Replace original categories columns with the newly generated ones\n",
    "        5. Check and remove duplicated row if there is\n",
    "    \n",
    "    \"\"\"\n",
    "    # 1. Create a dataframe of the 36 individual category columns\n",
    "    categories = df.categories.str.split(';', expand=True)\n",
    "    \n",
    "    # 2. Generate Y columns name and rename columns\n",
    "    category_colnames = [i[:-2] for i in categories.iloc[0, :]]\n",
    "    categories.columns = category_colnames\n",
    "    \n",
    "    # 3. Clean columns values\n",
    "    for column in categories:\n",
    "        # set each value to be the last character of the string\n",
    "        categories[column] = categories[column].str[-1]\n",
    "\n",
    "        # convert column from string to numeric\n",
    "        categories[column] = pd.to_numeric(categories[column], errors='coerce')\n",
    "        \n",
    "    # 4. Use new categories to replace the original column from `df`\n",
    "    df.drop(['categories'], axis=1, inplace=True)\n",
    "    df = pd.concat([df, categories], axis=1)\n",
    "    \n",
    "    # 5. Check number of duplicates\n",
    "    if df.duplicated().sum() != 0:\n",
    "        df = df.drop_duplicates()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(df, database_filename):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        1. df: the cleaned dataframe, which is generated by the clean data function\n",
    "        2. database_filename: name of the database file\n",
    "    Output:\n",
    "        Saved database file\n",
    "        \n",
    "    Process:\n",
    "        1. Create engine for saving dataframe\n",
    "        2. Use to_sql method to dump dataframe to file\n",
    "    \"\"\"\n",
    "    \n",
    "    engine = create_engine('sqlite:///{}.db'.formate(database_filename))\n",
    "    df.to_sql(database_filename, engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    if len(sys.argv) == 4:\n",
    "\n",
    "        messages_filepath, categories_filepath, database_filepath = sys.argv[1:]\n",
    "\n",
    "        print('Loading data...\\n    MESSAGES: {}\\n    CATEGORIES: {}'\n",
    "              .format(messages_filepath, categories_filepath))\n",
    "        df = load_data(messages_filepath, categories_filepath)\n",
    "\n",
    "        print('Cleaning data...')\n",
    "        df = clean_data(df)\n",
    "        \n",
    "        print('Saving data...\\n    DATABASE: {}'.format(database_filepath))\n",
    "        save_data(df, database_filepath)\n",
    "        \n",
    "        print('Cleaned data saved to database!')\n",
    "    \n",
    "    else:\n",
    "        print('Please provide the filepaths of the messages and categories '\\\n",
    "              'datasets as the first and second argument respectively, as '\\\n",
    "              'well as the filepath of the database to save the cleaned data '\\\n",
    "              'to as the third argument. \\n\\nExample: python process_data.py '\\\n",
    "              'disaster_messages.csv disaster_categories.csv '\\\n",
    "              'DisasterResponse.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the filepaths of the messages and categories datasets as the first and second argument respectively, as well as the filepath of the database to save the cleaned data to as the third argument. \n",
      "\n",
      "Example: python process_data.py disaster_messages.csv disaster_categories.csv DisasterResponse.db\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
